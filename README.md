# Protein-to-Drug Prediction with LLM

## Project Overview
This project aims to generate drug candidates from protein sequences using large language models (LLMs) and the TCR-Epitope dataset. The main objective is to fine-tune an LLM to predict peptide sequences that bind effectively to given protein targets. The second part of the project evaluates the quality of the generated sequences by using a classifier to predict binding efficacy.

## Part 1: Fine-Tuning a Large Language Model (LLM)

### Overview
The first part of this project focuses on using a large language model to predict peptide sequences from protein target sequences. The LLM is fine-tuned to understand the relationships between protein sequences and their potential binding peptides, enabling it to generate biologically potent candidates. This approach leverages recent advances in natural language processing (NLP) and applies them to bioinformatics.

### Key Features
- **Data Processing**: The `data_builder.py` module prepares the datasets required for fine-tuning. It reads input files containing protein and peptide sequences, specifically the TCR-Epitope dataset, and transforms them into tokenized formats suitable for LLM training.
- **Sequence Prediction and Fine-Tuning**: The `fineTuning.py` script fine-tunes a pre-trained language model, Mistral-7B, using LoRA (Low-Rank Adaptation) configurations. This parameter-efficient fine-tuning allows the model to adapt specifically to the sequence prediction task while keeping most of the original model's knowledge intact.
- **Training Process**: The fine-tuning process is configured to use appropriate hyperparameters such as batch size, learning rate, and epochs, ensuring effective training for peptide sequence generation based on given protein sequences. The `SFTTrainer` is used to handle the training routine efficiently.
- **Embedding Generation**: After fine-tuning, the `embedding_generator.py` script is used to generate sequence embeddings. These embeddings represent the learned features of the protein-peptide sequences and are used in downstream tasks, such as classification.

### How to Fine-Tune the Model
1. **Dataset Preparation**: Use `data_builder.py` to create the training, validation, and test datasets.
2. **Fine-Tuning**: Run `fineTuning.py` to fine-tune the LLM (Mistral-7B) on the prepared dataset.
3. **Embedding Generation**: Once fine-tuning is complete, use `embedding_generator.py` to generate sequence embeddings for further analysis.

## Part 2: Developing the Classifier

### Overview
The second part of the project involves developing classifiers to determine the efficacy of the protein-drug interactions generated by the LLM. These classifiers utilize the embeddings generated from the fine-tuned LLM to predict binding efficacy, providing a quantitative measure of how well a given peptide sequence binds to a protein target. This evaluation is crucial in assessing the performance and practicality of the LLM-generated drug candidates.

### Key Features
- **Embedding-Based Classification**: The embeddings generated from the fine-tuned LLM serve as input features for the classifier. These embeddings capture the essential features of protein and peptide sequences, allowing the classifier to make informed predictions.
- **Classifier Implementation**: The `classifier.py` script uses XGBoost as the main classifier. XGBoost is a powerful gradient boosting algorithm known for its accuracy and efficiency. The classifier is trained on the embeddings to predict the binding efficacy of the protein-peptide interactions.
- **Training & Evaluation**: The classifier is trained incrementally to handle large datasets, and its performance is evaluated using metrics like accuracy, precision, recall, and ROC AUC. The `sequence_classifier.py` script provides an alternative approach, where a language model is directly fine-tuned for sequence classification.

### How to Develop the Classifier
1. **Generate Embeddings**: Use the fine-tuned LLM to generate embeddings for the sequences as described above.
2. **Train Classifier**: Run `classifier.py` to train the XGBoost model using the generated embeddings. Alternatively, use `sequence_classifier.py` to train a model directly on tokenized sequences.
3. **Evaluate the Model**: Evaluate the trained classifier using various metrics to assess its performance in predicting the quality of protein-peptide interactions.

## Dataset
The datasets used are based on protein and peptide sequences that describe the interactions between various protein chains and corresponding drug candidates.
- Input formats include protein sequences (`chain_a`) and peptide sequences (`chain_b`).
- Labels indicate binding efficacy, which is predicted during classification.


